1) Natural Language processing enables computer to understand and process human languages which include both text and audio.

2) Text Cleaning: 
       a) Text Standardizatoin : 
                       Convert to Lower Case
                       Spelling Correction     : pyspellchecker
                       
       b) Eliminate Undesirable Items From Text :
                      Removing Additional Spaces
                      Removing Punctuations 
                      Removing URLs 
                      Removing Digits 
                      Removing Stopwords        : from nltk.corpus import stopwords
                      
       
       c)  Convert Non-Words to Words 
                      Convert Emoji Into Words     : convert emoji into words
                      
       d) Convert Negative Word to its Antonyms  :  For instance 'not good’ should be replaced with bad.
       
       
       e) Dealing With Base and Derived Words 
                     i) Stemming :  Stemming is a rule base technique. 
                                 :  For instance 'going' and 'gone' will get converted to 'go' but 'went' will not.
                                 
                     ii) Lemmatization : Lemmatization is a dictionary based technique so it is slow
                                         more accurate than stemming.
                                         lemmatization requires Parts of speech tagging.
                                         
                     iii) Parts of Speech (POS) tagging :             nltk.pos_tag()
                     
                     
                     
 
 3) Text to Numeric Conversion:
          a) few common NLP vocabulary terms: 
                       i) Corpus  : Collection of all available textual data is known as corpus. 
                                  : Corpus text = trainig text + testing text 
                       
                       ii) Tokenization: Text is segmented into its smaller parts called tokens. 
                                          This segmentation process is known as tokenization.
                                          However tokens can be something else also such as combination of 2 words. 
                       
                       iii) Bag of Words (BOW) : It does not consider meaning or context of the word in the document.
                       
                       iv) N-Gram : N-grams is a contiguous sequence of n words from a given sample of source text.
                       
          
          b) Conversion : 
                       i) CountVectorization : converts a collection of text documents to a matrix of token counts. 
                                               Total no of tokens i.e. vocabulary size will equal to matrix column length where each column represents a token. 
                                               Number at a cell will indicate count of that token in the record.
                                               Results in biasing in favour of most frequent words.
                                               This ends up in ignoring rare words which could have helped is in processing our data more efficiently.
                     
                     
                       ii) TF-IDF Vectorization : In this  we consider overall document weightage of a word.
                       .                          It helps us in dealing with most frequent words. Using it we can penalize them. 
                       
                    
                    
          c)  Post Conversion : Both Countvectorization and tf-idf technique produce large size matrix. 
                              : Use dimension reduction technique.
                              : Use Truncated SVD:  Performs linear dimensionality reduction by means of truncated singular value decomposition (SVD).
                                                    It efficiently works on sparse matrices.  
                                
                                
                                
                                
                                
                                
                                
4) Advanced text to numeric conversion techniques:
           a) Word Embedding : also known as Word Vectorization. 
                             : It means converting word into vector. 
                             
           Question) Why we need word embeddings?
           Answer) A problem with our previous text to numeric conversion techniques was that they ignore synonyms for example 
                   word 'measure' and ‘calculate’ were represented differently however in most sentences they can be used 
                   interchangeably.
                   In Word Embedding similar words are spatially close to each other in vector space. 
                   Word Embedding is also capable of preserving semantic and syntactic similarity and relation with other words. 
                   
                   For instance, by adding a “female” vector to the vector “king”, we obtain the vector “queen” and by 
                   adding a “plural” vector to the vector “king”, we obtain “kings”.          
                             
                   Another problem we observe was of high dimensionality sparse matrix. 
                   Word Embedding produces low dimensionality dense matrix.          
                             
                             
                             
             b) Some Common NLP terms:
                   i) Dimensionality  : Dimensionality refers to the length of vectors. 
                   ii)  Padding : Padding is task of appending string up to given specific length with whitespaces.
                                  Padding is used to represent all records as fixed length.          
                             
                   iii) Euclidean Distance : Euclidean distance is the shortest distance between two points in (Euclidean) space.
                   iv) Cosine Similarity :   Cosine similarity is a measure of similarity between two nonzero vectors of an inner product space. 
                                             It measures the cosine of the angle between them.
                   
                  
                  
                  
              
              c) Word Embedding Techniques(Word to vector conversion techniques): 
                   i) Word2Vec: Each unique word in the corpus is assigned a corresponding vector in the space. 
                              : Relies only on local information of language hence the semantics learnt for a given word is 
                                only affected by the surrounding words. 
                              : Drawback of word2vec is its unablity to takecare of OOV word.  
                   
                              : 2 flavours: 
                                       i)  skip-gram : Designed to predict the context from base word.
                                                     : From a given word, Skip-gram model tries to predict its neighbouring words.
                                       
                                       ii) continuous bag of words(CBOW) : Designed to predict the base(target) word from context.
                                                                           CBOW is faster to train than the skip-gram and gives slightly better accuracy for the frequent words.
                                       
                                       eg) jay was hit by a red bus in:(if window =2)
                                               a) skipgram -->> input:  red  red red red  (4 samples for one input red)
                                                                outut:  by    a   bus in 
                                       
                                               b)CBOW -->> input:   by a bus in (1 sample for input 'by a bus in')
                                                           output : red
                                                           
                                                           
                                                           
                                      iii) Negative sampling : Adding some random samples whose target is zero.
                                                             : example-> for above example in skip gram there is:(converted our model from neural network to logistic regression 
                                                                         i.e. as classification problem and thus the step that was, project to output vocabulary , that was so costly has been removed)
                                                                                 
                                                                                 input   : red red red red red red red 
                                                                                  output  : by   a  bus  in jay  was hit
                                                                                  target  : 1    1   1   1   0    0   0 
                                                                                  
                                                                                  
                                                              : It is done because if all targets will be of one class only then learning will be zero 
                                                                as model will always see only one target and cannnot learn to differentiate.
                                                           
                                                           
                                         
                                         
                                         
                                         
                                      
                                      
                                      
                                      
                    ii) GLoVE : Glove embedding technique is based on (first) construction of a co-occurrence matrix from a training corpus 
                                and then (second) factorization of co-occurrence matrix in order to yield word vector. 
                    
                              : Unlike word2vec which captures only local statistics of token Glove captures
                                both global statistics and local statistics of a text tokens. 
                              : Its embeddings relate to the probabilities that two words appear together. 
                    
                    
                    iii) FastText : Extension of word2vec library.
                                  : For previously unseen words, typo errors, and OOV (Out Of Vocabulary) words the model
                                    can make an educated guess towards its meaning.Obvious trade off is processing time.
                                   
                                  : Each word is represented as a bag of character n-grams, so the overall word embedding is 
                                    a sum of these character n-grams. 
                                    
                                  : As a simple example, if we set n=3, the vector for the word “where” would be represented
                                    by a sum of trigrams: <wh, whe, her, ere, re>  
                                    
                                    
                                    
                                
                                     
                d) Text to Numeric Convertion Using Word Vectors  : Two popular texts to numerical conversion techniques using word vectors :                        
                                      i) Vector Averaging : Averaging is preferred choice when we intend to use ML models such as lr, svm, gbm etc. 
                                     ii) create Embedding Matrix and then generate Keras Embedding layer

                                      
                                                           
                e) Apply ML/DL models.
                              :  Now fit embedding layer in sequential model followed by another layers such as LSTM,flatten, etc....
                              :  Now compile model and fit for training.    
                
      
      
      
  5) BERT - A new technique
